# Azure DICOM Service Emulator - Project Setup Design

**Date:** 2026-02-14
**Status:** Approved
**Goal:** Set up complete project structure and Docker containerization for local development

## Overview

Transform the existing Python files into a properly structured, containerized application that runs locally with Docker Compose. Use modern Python tooling (Python 3.12, uv, pyproject.toml) for fast builds and developer experience.

## Decisions Made

### Approach
**Incremental setup:** Complete project structure and Docker environment first, then implement features one at a time with working tests at each step.

### Technology Stack
- **Python:** 3.12 (latest stable, best performance and error messages)
- **Dependency Manager:** uv (10-100x faster than pip/Poetry, production-ready)
- **Package Format:** pyproject.toml (modern Python standard)
- **Container:** Multi-stage Dockerfile with Python 3.12-slim base

## Project Structure

```
azure-dicom-service-emulator/
├── app/
│   ├── __init__.py
│   ├── database.py              # NEW: SQLAlchemy async setup
│   ├── models/
│   │   ├── __init__.py
│   │   └── dicom.py             # MOVE: from root dicom.py
│   ├── routers/
│   │   ├── __init__.py
│   │   ├── dicomweb.py          # MOVE: from root dicomweb.py
│   │   ├── changefeed.py        # NEW: Change feed API
│   │   ├── extended_query_tags.py  # NEW: EQT management
│   │   └── operations.py        # NEW: Async operations
│   └── services/
│       ├── __init__.py
│       ├── dicom_engine.py      # MOVE: from root dicom_engine.py
│       └── multipart.py         # NEW: Multipart/related parser
├── main.py                       # KEEP: FastAPI app entry
├── Dockerfile                    # NEW: Multi-stage build
├── docker-compose.yml            # KEEP: Already correct
├── pyproject.toml                # NEW: Dependencies via uv
├── uv.lock                       # NEW: Generated by uv
├── .python-version               # NEW: Pin to 3.12
├── smoke_test.py                 # KEEP: In root for easy access
├── README.md                     # KEEP
├── CLAUDE.md                     # KEEP: Just created
└── LICENSE                       # KEEP
```

### Migration Strategy
1. Create `app/` directory structure
2. Move existing files into appropriate subdirectories
3. Create new modules (database.py, multipart.py, missing routers)
4. Update imports if needed

## Docker Build Strategy

### Multi-Stage Dockerfile

**Stage 1 - Builder:**
```dockerfile
FROM python:3.12-slim AS builder
WORKDIR /app
# Install uv
COPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv
# Copy dependency files
COPY pyproject.toml uv.lock ./
# Install dependencies to virtual environment
RUN uv sync --frozen --no-dev
```

**Stage 2 - Runtime:**
```dockerfile
FROM python:3.12-slim
WORKDIR /app
# Copy virtual environment from builder
COPY --from=builder /app/.venv /app/.venv
# Copy application code
COPY app/ /app/app/
COPY main.py /app/
# Set environment
ENV PATH="/app/.venv/bin:$PATH"
ENV PYTHONUNBUFFERED=1
# Health check
HEALTHCHECK --interval=10s --timeout=5s --retries=5 \
  CMD curl -f http://localhost:8080/health || exit 1
# Run with uvicorn
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8080"]
```

### Benefits
- **Fast rebuilds:** Dependencies cached in builder stage
- **Small image:** Runtime stage only has essentials (~300-400MB)
- **uv speed:** 10-100x faster dependency resolution
- **Platform:** Builds for linux/amd64 (required for cloud deployment)

## Missing Modules Implementation

### app/database.py
SQLAlchemy async engine setup:

```python
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.ext.asyncio import async_sessionmaker
from sqlalchemy.orm import declarative_base
import os

DATABASE_URL = os.getenv("DATABASE_URL",
    "postgresql+asyncpg://emulator:emulator@postgres:5432/dicom_emulator")

engine = create_async_engine(DATABASE_URL, echo=False)
AsyncSessionLocal = async_sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)
Base = declarative_base()

async def get_db():
    async with AsyncSessionLocal() as session:
        yield session
```

### app/services/multipart.py
Simple `multipart/related` parser for DICOM uploads. Parses boundary from Content-Type header, extracts binary DICOM parts, returns list of data chunks.

### Missing Routers (Minimal Implementation)

**app/routers/changefeed.py:**
- `GET /changefeed`: Query ChangeFeedEntry table with optional startTime/endTime filters
- `GET /changefeed/latest`: Return most recent entry by sequence number

**app/routers/extended_query_tags.py:**
- `GET /extendedquerytags`: List all tags
- `POST /extendedquerytags`: Create new tag(s), return operation ID
- `GET /extendedquerytags/{path}`: Get specific tag
- `DELETE /extendedquerytags/{path}`: Remove tag

**app/routers/operations.py:**
- `GET /operations/{id}`: Return operation status by UUID

**Strategy:** Minimal but functional - basic CRUD using existing SQLAlchemy models. No complex business logic initially. Just enough to pass smoke tests.

## Dependencies (pyproject.toml)

```toml
[project]
name = "azure-dicom-service-emulator"
version = "0.1.0"
description = "Local emulator for Azure Health Data Services DICOM Service"
requires-python = ">=3.12"
dependencies = [
    "fastapi>=0.115.0",
    "uvicorn[standard]>=0.32.0",
    "sqlalchemy[asyncio]>=2.0.36",
    "asyncpg>=0.30.0",
    "pydicom>=3.0.1",
    "httpx>=0.28.0",
]

[project.optional-dependencies]
dev = [
    "ruff>=0.8.0",
    "mypy>=1.13.0",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"
```

### Key Dependencies
- **FastAPI + uvicorn:** Web framework and ASGI server
- **SQLAlchemy + asyncpg:** Async ORM and PostgreSQL driver
- **pydicom:** DICOM file parsing
- **httpx:** HTTP client for testing

### Versioning Strategy
- Use `>=` with minimum versions for flexibility
- `uv.lock` ensures reproducible builds
- All packages are mainstream with excellent support

## Testing and Validation

### Stage 1 - Project Structure
```bash
# Test imports after reorganization
python -c "from app.database import engine"
python -c "from app.routers import dicomweb, changefeed"
```

### Stage 2 - Docker Build
```bash
# Build (should be fast with uv)
docker compose build

# Verify image size (~300-400MB)
docker images azure-dicom-service-emulator
```

### Stage 3 - Container Health
```bash
# Start services
docker compose up -d

# Check health
docker compose ps  # All should be "healthy"
curl http://localhost:8080/health
```

### Stage 4 - Smoke Tests
```bash
# Run full test suite
python smoke_test.py http://localhost:8080
# Expected: ALL TESTS PASSED (8 sections)
```

### Success Criteria
- ✅ All imports resolve
- ✅ Docker builds in <2 minutes
- ✅ Services start and pass health checks
- ✅ All 8 smoke test sections pass

### Failure Handling
Minimal implementation approach allows quick iteration. If tests fail, we can rapidly fix individual routers without major rewrites.

## Implementation Order

1. **Project structure:** Create directories, move files
2. **New modules:** database.py, multipart.py
3. **Missing routers:** changefeed, extended_query_tags, operations (minimal)
4. **Dependencies:** pyproject.toml, .python-version
5. **Docker:** Dockerfile with multi-stage build
6. **Validation:** Test at each stage per strategy above

## Why This Design

**uv over Poetry/pip:**
- Emulators rebuild frequently during dev
- 10-100x speed advantage matters
- All dependencies are mainstream (no compatibility risk)
- Simpler than Poetry, faster than pip

**Python 3.12:**
- Latest stable with best performance
- Excellent error messages for debugging
- Full cloud platform support

**Multi-stage Docker:**
- Fast rebuilds (cached dependencies)
- Small images (no build tools in runtime)
- Production-ready pattern

**Minimal routers:**
- Get to working state quickly
- Validate architecture early
- Iterate on features incrementally

## Next Steps

After design approval:
1. Create implementation plan with detailed tasks
2. Set up git worktree for isolated development
3. Execute implementation with testing at each stage
4. Run smoke tests to validate
5. Commit working setup to main branch
